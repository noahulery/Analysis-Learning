{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# this will collect the url for every year page and store it in year_links\n",
    "first_url = 'https://www.foxsports.com/nfl/stats?season=2019&week=101&category=RUSHING'\n",
    "searching_url = first_url\n",
    "url_client = urlopen(searching_url)\n",
    "html_url = url_client.read()\n",
    "url_client.close()\n",
    "soup_url = soup(html_url, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This finds the name of all the stats and removes the first column which is just \"Rushing\", creates a dictionary of all the stats\n",
    "stats = soup_url.table.findAll('th')\n",
    "stats.pop(0)\n",
    "rush_stats = ['Player Name']\n",
    "\n",
    "for stat in stats:\n",
    "    rush_stats.append(stat.text.strip())\n",
    "    \n",
    "stat_dict = {}\n",
    "for stat in rush_stats:\n",
    "    stat_dict[stat] = []\n",
    "\n",
    "\n",
    "\n",
    "# Finds all names, reorders them, puts them in a list\n",
    "\n",
    "names = soup_url.table.tbody.findAll('a')\n",
    "name_list = []\n",
    "\n",
    "for name in names:\n",
    "    if name.span == None:\n",
    "        names.remove(name)\n",
    "    if name.a != None:\n",
    "        names.remove(name)\n",
    "        \n",
    "for name in names:\n",
    "    name = name.span.text\n",
    "    temp = name.split()\n",
    "    if len(temp) == 2:\n",
    "        temp[0] = temp[0].replace(',', '')\n",
    "        new = temp[1] + ' ' + temp[0]\n",
    "        name_list.append(new)\n",
    "    elif len(temp) == 3:\n",
    "        temp[1] = temp[1].replace(',', '')\n",
    "        new = temp[2] + ' ' + temp[0] + ' ' + temp[1]\n",
    "        name_list.append(new)\n",
    "    else:\n",
    "        print(len(temp))    \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# Finds and formats all stat values, long rushes are done separately due to the HTML formatting\n",
    "# All stats are separated into their own lists\n",
    "\n",
    "numbers = soup_url.table.tbody.findAll('td', {'class' : 'wisbb_priorityColumn'})\n",
    "\n",
    "\n",
    "stat_vals = []\n",
    "\n",
    "for number in numbers:\n",
    "    number = number.text\n",
    "    stat_vals.append(float(number))\n",
    "    \n",
    "att = stat_vals[::4]\n",
    "yards = stat_vals[1::4]\n",
    "avg = stat_vals[2::4]\n",
    "td = stat_vals[3::4]\n",
    "\n",
    "longs = soup_url.table.tbody.findAll('td', {'class' : None})\n",
    "lng = []\n",
    "\n",
    "for long in longs:\n",
    "    long = float(long.text)\n",
    "    lng.append(long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "rushing_week1_2019 = pd.DataFrame()\n",
    "count = 0\n",
    "for _ in range(50):\n",
    "    stat_dict['Player Name'] = name_list[count]\n",
    "    stat_dict['Att'] = att[count]\n",
    "    stat_dict['Yds'] = yards[count]\n",
    "    stat_dict['Avg'] = avg[count]\n",
    "    stat_dict['TD'] = td[count]\n",
    "    stat_dict['Lng'] = lng[count]\n",
    "    rushing_week1_2019 = rushing_week1_2019.append(stat_dict, ignore_index = True)\n",
    "    count += 1\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "rushing_week1_2019.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
