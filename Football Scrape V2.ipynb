{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# this will collect the url for every year page and store it in year_links\n",
    "\n",
    "first_url = 'https://www.pro-football-reference.com/years' # have to strat from here to get the years but this cant be your base url to concat to.\n",
    "searching_url = first_url\n",
    "url_client = urlopen(searching_url)\n",
    "html_url = url_client.read()\n",
    "url_client.close()\n",
    "soup_url = soup(html_url, 'html.parser')\n",
    "\n",
    "year_page_links = soup_url.body.table.findAll('a')\n",
    "year_links = []\n",
    "\n",
    "for link in year_page_links:\n",
    "    if '201' in link.text: # pulls from 2010 to 2019\n",
    "        year_links.append(link['href'])      \n",
    "\n",
    "\n",
    "# this will itterate through every year collecting weeks then CSVs\n",
    "for year_link in year_links:\n",
    "    \n",
    "    base_url = 'https://www.pro-football-reference.com'\n",
    "    searching_url = base_url + year_link\n",
    "    url_client = urlopen(searching_url)\n",
    "    html_url = url_client.read()\n",
    "    url_client.close()\n",
    "    soup_url = soup(html_url, 'html.parser')\n",
    "    \n",
    "    print('I am currently reading {a}'.format(a=searching_url))\n",
    "    \n",
    "    week_page_links = soup_url.findAll('a')\n",
    "    week_links = []\n",
    "\n",
    "    for link in week_page_links:\n",
    "        if 'Week' in link.text and 'Weeks' not in link.text and 'This Week' not in link.text:\n",
    "            if link['href'] not in week_links:\n",
    "                week_links.append(link['href'])\n",
    "        \n",
    "    for week_link in week_links:\n",
    "        \n",
    "        base_url = 'https://www.pro-football-reference.com'\n",
    "        searching_url = base_url + week_link\n",
    "        url_client = urlopen(searching_url)\n",
    "        html_url = url_client.read()\n",
    "        url_client.close()\n",
    "        soup_url = soup(html_url, 'html.parser')\n",
    "        \n",
    "        print('I am currently reading {a}'.format(a=searching_url))\n",
    "        \n",
    "        boxscore_page_links = soup_url.body.findAll('a')\n",
    "        boxscore_links = []\n",
    "        for link in boxscore_page_links:\n",
    "            if 'boxscores/2' in link['href']:\n",
    "                boxscore_links.append(link['href'])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "for boxscore_link in boxscore_links:\n",
    "    \n",
    "    base_url = 'https://www.pro-football-reference.com'\n",
    "    searching_url = base_url + boxscore_link\n",
    "    url_client = urlopen(searching_url)\n",
    "    html_url = url_client.read()\n",
    "    url_client.close()\n",
    "    soup_url = soup(html_url, 'html.parser')\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I have set this up to iterate across all years from 2010 to 2019\n",
    "## it will iterate across each week of each season. In each week it will identify the box scores for every game\n",
    "### -\n",
    "\n",
    "### for each box score there is a table containing all the offensive stats. I need to figure out the best way to scrape this\n",
    "### -\n",
    "### -\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### I can try to use the built in convert to CSV and pull it or i can pull it directly from the table as it exists\n",
    "# -\n",
    "# -\n",
    "# -\n",
    "### after this I will need to figure out the best way to combine the stats for each player across each week (weeks must be retained for time series)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
